
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import torch\n",
        "import gradio as gr\n",
        "import nltk\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "\n",
        "# ---------------------------\n",
        "# Global models (load once)\n",
        "# ---------------------------\n",
        "BERT_MODEL_NAME = \"google-bert/bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
        "bert_model = AutoModelForQuestionAnswering.from_pretrained(BERT_MODEL_NAME)\n",
        "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# ---------------------------\n",
        "# Data processing\n",
        "# ---------------------------\n",
        "def process_url(url_link):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "                      \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                      \"Chrome/115.0.0.0 Safari/537.36\"\n",
        "    }\n",
        "    response = requests.get(url_link, headers=headers)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    paragraphs = [p.get_text().strip() for p in soup.find_all(\"p\") if len(p.get_text().strip()) > 30]\n",
        "    text = \" \".join(paragraphs)\n",
        "    if not text:\n",
        "        text = soup.get_text(separator=\" \", strip=True)\n",
        "    return text\n",
        "\n",
        "def chunk_by_sentences(text, max_tokens=400, overlap=50):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    chunks, current_chunk = [], []\n",
        "    current_length = 0\n",
        "\n",
        "    for sent in sentences:\n",
        "        sent_tokens = bert_tokenizer.tokenize(sent)\n",
        "        if current_length + len(sent_tokens) > max_tokens:\n",
        "            chunks.append(bert_tokenizer.convert_tokens_to_string(sum(current_chunk, [])))\n",
        "            current_chunk = current_chunk[-overlap:]  # keep overlap\n",
        "            current_length = len(current_chunk)\n",
        "        current_chunk.append(sent_tokens)\n",
        "        current_length += len(sent_tokens)\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(bert_tokenizer.convert_tokens_to_string(sum(current_chunk, [])))\n",
        "    if not chunks:\n",
        "        chunks = [text]  # fallback if text too short\n",
        "    return chunks\n",
        "\n",
        "def embeddings_vector_store(chunks):\n",
        "    if not chunks:\n",
        "        raise ValueError(\"No chunks were generated from the text!\")\n",
        "    vectors = embedding_model.encode(chunks, convert_to_numpy=True)\n",
        "    if vectors.ndim == 1:\n",
        "        vectors = np.expand_dims(vectors, axis=0)\n",
        "    dim = vectors.shape[1]\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "    index.add(vectors)\n",
        "    return index, chunks, embedding_model\n",
        "\n",
        "def retriever(vectorstore, question, k=8):\n",
        "    index, chunks, emb_model = vectorstore\n",
        "    query_vector = emb_model.encode([question])\n",
        "    distances, indices = index.search(query_vector, k)\n",
        "    retrieved_chunks = [chunks[i] for i in indices[0] if i < len(chunks)]\n",
        "    return retrieved_chunks\n",
        "\n",
        "# ---------------------------\n",
        "# QA with BERT\n",
        "# ---------------------------\n",
        "def run_bert(question, chunks):\n",
        "    best_answer = \"\"\n",
        "    best_score = -float('inf')\n",
        "    for chunk in chunks:\n",
        "        inputs = bert_tokenizer.encode_plus(\n",
        "            question,\n",
        "            chunk,\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=512,\n",
        "            truncation=True\n",
        "        )\n",
        "        input_ids = inputs[\"input_ids\"]\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = bert_model(input_ids, token_type_ids=token_type_ids)\n",
        "            start_scores = outputs.start_logits\n",
        "            end_scores = outputs.end_logits\n",
        "\n",
        "        start_idx = torch.argmax(start_scores)\n",
        "        end_idx = torch.argmax(end_scores)\n",
        "\n",
        "        answer = bert_tokenizer.convert_tokens_to_string(\n",
        "            bert_tokenizer.convert_ids_to_tokens(input_ids[0][start_idx:end_idx+1])\n",
        "        )\n",
        "        score = start_scores[0][start_idx] + end_scores[0][end_idx]\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_answer = answer\n",
        "\n",
        "    return best_answer\n",
        "\n",
        "# ---------------------------\n",
        "# Main pipeline\n",
        "# ---------------------------\n",
        "def answer_question_from_url(url, question, chunk_size=400, chunk_overlap=50, top_k=3):\n",
        "    text = process_url(url)\n",
        "    if not text.strip():\n",
        "        return \"‚ùå Could not extract text from the URL.\", []\n",
        "\n",
        "    chunked_text = chunk_by_sentences(text, chunk_size, chunk_overlap)\n",
        "    vectorstore = embeddings_vector_store(chunked_text)\n",
        "    retrieved_chunks = retriever(vectorstore, question, top_k)\n",
        "\n",
        "    print(\"\\n--- Retrieved Chunks ---\\n\")\n",
        "    for i, c in enumerate(retrieved_chunks, 1):\n",
        "        print(f\"Chunk {i}:\\n{c[:300]}...\\n\")\n",
        "\n",
        "    answer = run_bert(question, retrieved_chunks)\n",
        "    return answer, retrieved_chunks\n",
        "\n",
        "# ---------------------------\n",
        "# Gradio UI\n",
        "# ---------------------------\n",
        "with gr.Blocks(title=\"Websites Analyzer for Lazy Readers\", theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"üìÑüîé **RAG website:** drop your URL, ask a question, read the answer ‚Äî it's that easy!\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            url_input = gr.Textbox(label=\"Website URL\")\n",
        "            query_input = gr.Textbox(label=\"Ask a question about the WEBSITE\")\n",
        "            top_k_slider = gr.Slider(1, 10, value=3, step=1, label=\"Top-K Chunks\")\n",
        "            ask_btn = gr.Button(\"Answer\", variant=\"primary\")\n",
        "            clear_btn = gr.Button(\"Clear Models (free VRAM)\")\n",
        "            status = gr.Markdown(\"Status: _waiting for URL_\")\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            answer_output = gr.Markdown(label=\"Answer\")\n",
        "            with gr.Accordion(\"Show retrieved chunks\", open=False):\n",
        "                chunks_view = gr.Markdown()\n",
        "\n",
        "    def answer_query(url, question, top_k):\n",
        "        answer, retrieved = answer_question_from_url(url, question, top_k=top_k)\n",
        "        retrieved_text = \"\\n\\n---\\n\\n\".join(retrieved)\n",
        "        return answer, retrieved_text, \"‚úÖ Done!\"\n",
        "\n",
        "    def reset_models():\n",
        "        return \"\", \"\", \"Models cleared! VRAM freed.\"\n",
        "\n",
        "    ask_btn.click(\n",
        "        fn=answer_query,\n",
        "        inputs=[url_input, query_input, top_k_slider],\n",
        "        outputs=[answer_output, chunks_view, status]\n",
        "    )\n",
        "\n",
        "    clear_btn.click(\n",
        "        fn=reset_models,\n",
        "        inputs=[],\n",
        "        outputs=[answer_output, chunks_view, status]\n",
        "    )\n",

        "demo.queue().launch(debug=True)\n",

      
      
