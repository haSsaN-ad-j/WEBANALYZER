{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZXk9OlE1LNq",
        "outputId": "87352407-f362-484f-8c90-6aaa66fc4653"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.3.29)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (0.3.32)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.101.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.5)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.75)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.9)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.16)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.7)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.11.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.24.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.6.7->langchain-community) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-community langchain-openai openai requests beautifulsoup4\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence-transformers\n"
      ],
      "metadata": {
        "id": "uXE-HZRzdUeS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPpSz2CpgQQ9",
        "outputId": "94d5b4e3-d1a1-4136-86b3-76beb06329bc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import torch\n",
        "import gradio as gr\n",
        "import nltk\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "\n",
        "# ---------------------------\n",
        "# Global models (load once)\n",
        "# ---------------------------\n",
        "BERT_MODEL_NAME = \"google-bert/bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME)\n",
        "bert_model = AutoModelForQuestionAnswering.from_pretrained(BERT_MODEL_NAME)\n",
        "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# ---------------------------\n",
        "# Data processing\n",
        "# ---------------------------\n",
        "def process_url(url_link):\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
        "                      \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                      \"Chrome/115.0.0.0 Safari/537.36\"\n",
        "    }\n",
        "    response = requests.get(url_link, headers=headers)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    paragraphs = [p.get_text().strip() for p in soup.find_all(\"p\") if len(p.get_text().strip()) > 30]\n",
        "    text = \" \".join(paragraphs)\n",
        "    if not text:\n",
        "        text = soup.get_text(separator=\" \", strip=True)\n",
        "    return text\n",
        "\n",
        "def chunk_by_sentences(text, max_tokens=400, overlap=50):\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    chunks, current_chunk = [], []\n",
        "    current_length = 0\n",
        "\n",
        "    for sent in sentences:\n",
        "        sent_tokens = bert_tokenizer.tokenize(sent)\n",
        "        if current_length + len(sent_tokens) > max_tokens:\n",
        "            chunks.append(bert_tokenizer.convert_tokens_to_string(sum(current_chunk, [])))\n",
        "            current_chunk = current_chunk[-overlap:]  # keep overlap\n",
        "            current_length = len(current_chunk)\n",
        "        current_chunk.append(sent_tokens)\n",
        "        current_length += len(sent_tokens)\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(bert_tokenizer.convert_tokens_to_string(sum(current_chunk, [])))\n",
        "    if not chunks:\n",
        "        chunks = [text]  # fallback if text too short\n",
        "    return chunks\n",
        "\n",
        "def embeddings_vector_store(chunks):\n",
        "    if not chunks:\n",
        "        raise ValueError(\"No chunks were generated from the text!\")\n",
        "    vectors = embedding_model.encode(chunks, convert_to_numpy=True)\n",
        "    if vectors.ndim == 1:\n",
        "        vectors = np.expand_dims(vectors, axis=0)\n",
        "    dim = vectors.shape[1]\n",
        "    index = faiss.IndexFlatL2(dim)\n",
        "    index.add(vectors)\n",
        "    return index, chunks, embedding_model\n",
        "\n",
        "def retriever(vectorstore, question, k=8):\n",
        "    index, chunks, emb_model = vectorstore\n",
        "    query_vector = emb_model.encode([question])\n",
        "    distances, indices = index.search(query_vector, k)\n",
        "    retrieved_chunks = [chunks[i] for i in indices[0] if i < len(chunks)]\n",
        "    return retrieved_chunks\n",
        "\n",
        "# ---------------------------\n",
        "# QA with BERT\n",
        "# ---------------------------\n",
        "def run_bert(question, chunks):\n",
        "    best_answer = \"\"\n",
        "    best_score = -float('inf')\n",
        "    for chunk in chunks:\n",
        "        inputs = bert_tokenizer.encode_plus(\n",
        "            question,\n",
        "            chunk,\n",
        "            return_tensors=\"pt\",\n",
        "            max_length=512,\n",
        "            truncation=True\n",
        "        )\n",
        "        input_ids = inputs[\"input_ids\"]\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = bert_model(input_ids, token_type_ids=token_type_ids)\n",
        "            start_scores = outputs.start_logits\n",
        "            end_scores = outputs.end_logits\n",
        "\n",
        "        start_idx = torch.argmax(start_scores)\n",
        "        end_idx = torch.argmax(end_scores)\n",
        "\n",
        "        answer = bert_tokenizer.convert_tokens_to_string(\n",
        "            bert_tokenizer.convert_ids_to_tokens(input_ids[0][start_idx:end_idx+1])\n",
        "        )\n",
        "        score = start_scores[0][start_idx] + end_scores[0][end_idx]\n",
        "\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_answer = answer\n",
        "\n",
        "    return best_answer\n",
        "\n",
        "# ---------------------------\n",
        "# Main pipeline\n",
        "# ---------------------------\n",
        "def answer_question_from_url(url, question, chunk_size=400, chunk_overlap=50, top_k=3):\n",
        "    text = process_url(url)\n",
        "    if not text.strip():\n",
        "        return \"‚ùå Could not extract text from the URL.\", []\n",
        "\n",
        "    chunked_text = chunk_by_sentences(text, chunk_size, chunk_overlap)\n",
        "    vectorstore = embeddings_vector_store(chunked_text)\n",
        "    retrieved_chunks = retriever(vectorstore, question, top_k)\n",
        "\n",
        "    print(\"\\n--- Retrieved Chunks ---\\n\")\n",
        "    for i, c in enumerate(retrieved_chunks, 1):\n",
        "        print(f\"Chunk {i}:\\n{c[:300]}...\\n\")\n",
        "\n",
        "    answer = run_bert(question, retrieved_chunks)\n",
        "    return answer, retrieved_chunks\n",
        "\n",
        "# ---------------------------\n",
        "# Gradio UI\n",
        "# ---------------------------\n",
        "with gr.Blocks(title=\"Websites Analyzer for Lazy Readers\", theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"üìÑüîé **RAG website:** drop your URL, ask a question, read the answer ‚Äî it's that easy!\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            url_input = gr.Textbox(label=\"Website URL\")\n",
        "            query_input = gr.Textbox(label=\"Ask a question about the WEBSITE\")\n",
        "            top_k_slider = gr.Slider(1, 10, value=3, step=1, label=\"Top-K Chunks\")\n",
        "            ask_btn = gr.Button(\"Answer\", variant=\"primary\")\n",
        "            clear_btn = gr.Button(\"Clear Models (free VRAM)\")\n",
        "            status = gr.Markdown(\"Status: _waiting for URL_\")\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            answer_output = gr.Markdown(label=\"Answer\")\n",
        "            with gr.Accordion(\"Show retrieved chunks\", open=False):\n",
        "                chunks_view = gr.Markdown()\n",
        "\n",
        "    def answer_query(url, question, top_k):\n",
        "        answer, retrieved = answer_question_from_url(url, question, top_k=top_k)\n",
        "        retrieved_text = \"\\n\\n---\\n\\n\".join(retrieved)\n",
        "        return answer, retrieved_text, \"‚úÖ Done!\"\n",
        "\n",
        "    def reset_models():\n",
        "        return \"\", \"\", \"Models cleared! VRAM freed.\"\n",
        "\n",
        "    ask_btn.click(\n",
        "        fn=answer_query,\n",
        "        inputs=[url_input, query_input, top_k_slider],\n",
        "        outputs=[answer_output, chunks_view, status]\n",
        "    )\n",
        "\n",
        "    clear_btn.click(\n",
        "        fn=reset_models,\n",
        "        inputs=[],\n",
        "        outputs=[answer_output, chunks_view, status]\n",
        "    )\n",
        "\n",
        "demo.queue().launch(debug=True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WD-DMbT9o_gY",
        "outputId": "ec6e5937-3174-4587-c91e-8fc44ef1449b"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "Some weights of the model checkpoint at google-bert/bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://d15046ce3e30eac51d.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d15046ce3e30eac51d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Retrieved Chunks ---\n",
            "\n",
            "Chunk 1:\n",
            "[ 9 ] [ 10 ] [ 11 ] [ 12 ] his maternal grandfather, joshua n. haldeman, who died in a plane crash when elon was a toddler, was an american - born canadian chiropractor, aviator and political activist in the technocracy movement [ 13 ] [ 14 ] who moved to south africa in 1950. [ 15 ] haldeman ' s an...\n",
            "\n",
            "Chunk 2:\n",
            "[ 31 ] elon has recounted trips to a wilderness school that he described as a \" paramilitary lord of the flies \" where \" bullying was a virtue \" and children were encouraged to fight over rations. [ 32 ] in one incident, after an altercation with a fellow pupil, elon was thrown down concrete steps a...\n",
            "\n",
            "Chunk 3:\n",
            "in early 2025, he served as senior advisor to united states president donald trump and as the de facto head of doge. after a public feud with trump, musk left the trump administration and announced he was creating his own political party, the america party. musk ' s political activities, views, and ...\n",
            "\n",
            "\n",
            "--- Retrieved Chunks ---\n",
            "\n",
            "Chunk 1:\n",
            "[ 9 ] [ 10 ] [ 11 ] [ 12 ] his maternal grandfather, joshua n. haldeman, who died in a plane crash when elon was a toddler, was an american - born canadian chiropractor, aviator and political activist in the technocracy movement [ 13 ] [ 14 ] who moved to south africa in 1950. [ 15 ] haldeman ' s an...\n",
            "\n",
            "Chunk 2:\n",
            "[ 31 ] elon has recounted trips to a wilderness school that he described as a \" paramilitary lord of the flies \" where \" bullying was a virtue \" and children were encouraged to fight over rations. [ 32 ] in one incident, after an altercation with a fellow pupil, elon was thrown down concrete steps a...\n",
            "\n",
            "Chunk 3:\n",
            "elon reeve musk frs ( / ÀàiÀêl…ín / ee - lon ; born june 28, 1971 ) is an international businessman and entrepreneur known for his leadership of tesla, spacex, x ( formerly twitter ), and the department of government efficiency ( doge ). musk has been the wealthiest person in the world since 2021 ; as ...\n",
            "\n",
            "\n",
            "--- Retrieved Chunks ---\n",
            "\n",
            "Chunk 1:\n",
            "elon reeve musk frs ( / ÀàiÀêl…ín / ee - lon ; born june 28, 1971 ) is an international businessman and entrepreneur known for his leadership of tesla, spacex, x ( formerly twitter ), and the department of government efficiency ( doge ). musk has been the wealthiest person in the world since 2021 ; as ...\n",
            "\n",
            "Chunk 2:\n",
            "elon reeve musk frs ( / ÀàiÀêl…ín / ee - lon ; born june 28, 1971 ) is an international businessman and entrepreneur known for his leadership of tesla, spacex, x ( formerly twitter ), and the department of government efficiency ( doge ). musk has been the wealthiest person in the world since 2021 ; as ...\n",
            "\n",
            "Chunk 3:\n",
            "elon reeve musk frs ( / ÀàiÀêl…ín / ee - lon ; born june 28, 1971 ) is an international businessman and entrepreneur known for his leadership of tesla, spacex, x ( formerly twitter ), and the department of government efficiency ( doge ). musk has been the wealthiest person in the world since 2021 ; as ...\n",
            "\n",
            "\n",
            "--- Retrieved Chunks ---\n",
            "\n",
            "Chunk 1:\n",
            "elon reeve musk frs ( / ÀàiÀêl…ín / ee - lon ; born june 28, 1971 ) is an international businessman and entrepreneur known for his leadership of tesla, spacex, x ( formerly twitter ), and the department of government efficiency ( doge ). musk has been the wealthiest person in the world since 2021 ; as ...\n",
            "\n",
            "Chunk 2:\n",
            "elon reeve musk frs ( / ÀàiÀêl…ín / ee - lon ; born june 28, 1971 ) is an international businessman and entrepreneur known for his leadership of tesla, spacex, x ( formerly twitter ), and the department of government efficiency ( doge ). musk has been the wealthiest person in the world since 2021 ; as ...\n",
            "\n",
            "\n",
            "--- Retrieved Chunks ---\n",
            "\n",
            "Chunk 1:\n",
            "elon reeve musk frs ( / ÀàiÀêl…ín / ee - lon ; born june 28, 1971 ) is an international businessman and entrepreneur known for his leadership of tesla, spacex, x ( formerly twitter ), and the department of government efficiency ( doge ). musk has been the wealthiest person in the world since 2021 ; as ...\n",
            "\n",
            "Chunk 2:\n",
            "elon reeve musk frs ( / ÀàiÀêl…ín / ee - lon ; born june 28, 1971 ) is an international businessman and entrepreneur known for his leadership of tesla, spacex, x ( formerly twitter ), and the department of government efficiency ( doge ). musk has been the wealthiest person in the world since 2021 ; as ...\n",
            "\n",
            "\n",
            "--- Retrieved Chunks ---\n",
            "\n",
            "Chunk 1:\n",
            "elon reeve musk frs ( / ÀàiÀêl…ín / ee - lon ; born june 28, 1971 ) is an international businessman and entrepreneur known for his leadership of tesla, spacex, x ( formerly twitter ), and the department of government efficiency ( doge ). musk has been the wealthiest person in the world since 2021 ; as ...\n",
            "\n",
            "Chunk 2:\n",
            "elon reeve musk frs ( / ÀàiÀêl…ín / ee - lon ; born june 28, 1971 ) is an international businessman and entrepreneur known for his leadership of tesla, spacex, x ( formerly twitter ), and the department of government efficiency ( doge ). musk has been the wealthiest person in the world since 2021 ; as ...\n",
            "\n",
            "\n",
            "--- Retrieved Chunks ---\n",
            "\n",
            "Chunk 1:\n",
            "[ 31 ] elon has recounted trips to a wilderness school that he described as a \" paramilitary lord of the flies \" where \" bullying was a virtue \" and children were encouraged to fight over rations. [ 32 ] in one incident, after an altercation with a fellow pupil, elon was thrown down concrete steps a...\n",
            "\n",
            "Chunk 2:\n",
            "in early 2025, he served as senior advisor to united states president donald trump and as the de facto head of doge. after a public feud with trump, musk left the trump administration and announced he was creating his own political party, the america party. musk ' s political activities, views, and ...\n",
            "\n",
            "Chunk 3:\n",
            "[ 296 ] a senate report alleged that musk could avoid up to $ 2 billion in legal liability as a result of doge ' s actions [ 311 ] in may 2025, bill gates accused musk of \" killing the world ' s poorest children \" through his cuts to usaid, [ 312 ] which modeling by boston university estimated had r...\n",
            "\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://d15046ce3e30eac51d.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    }
  ]
}